% !tex root=./notes.tex

\section{Algorithm}
\label{sec:algorithm}

Let $x$ be latent variables and $y$ be observed variables.
Consider a family of forward generative models $p_{\theta}(x, y)$ parameterized by $\theta$.
Let $q_{\phi}(x \given y)$ be a family of inference networks parameterized by $\phi$.
Given a set of unsupervised training data $(y^{(n)})_{n = 1}^N$ drawn from the true model $p_{\theta_{\text{true}}}(y)$, we are trying to
\begin{itemize}
    \item maximize $\frac{1}{N} \sum_{n = 1}^N \log p_{\theta}(y^{(n)})$ with respect to $\theta$, and
    \item minimize $\int p(y) d(q_{\phi}(x \given y), p_{\theta}(x \given y)) \,\mathrm dx$ with respect to $\phi$
\end{itemize}
where $d(\cdot, \cdot)$ is some distance between distributions.

We propose a two-step algorithm shown in Algorithm~\ref{alg:algorithm/cdae} which we call the \gls{CDAE}.
\begin{algorithm}
  \KwData{data $(y^{(n)})_{n = 1}^N$, generative model $p_{\theta}$, inference network $q_{\phi}$}
  \KwResult{learned parameters $\theta, \phi$}
  \Begin{
    Initialize $\phi, \theta$. \\
    \While{not converged}{
        Step 1. Optimize $\theta$: \\
        \begin{align}
            \maximize_{\theta} \left\{\frac{1}{N} \sum_{n = 1}^N \left[\int q_{\phi}(x \given y^{(n)}) \left[ \log p_{\theta}(x, y^{(n)}) - \log q_{\phi}(x \given y^{(n)}) \right] \,\mathrm dx\right]\right\}
        \end{align}
        by performing \gls{SGA}: \\
        \For{some number of iterations}{
            Sample $y^{(n)}$ from the dataset $(y^{(n)})_{n = 1}^N$. \\
            Sample $x \sim q_{\phi}(x \given y^{(n)})$. \\
            Calculate the gradient $\nabla_{\theta} p_{\theta}(x, y^{(n)})$. \\
            Update $\theta$.
        }
        Step 2. Optimize $\phi$: \\
        \begin{align}
            \minimize_{\phi} \left\{
                \int p_{\theta}(y) \KL{p_{\theta}(x \given y)}{q_{\phi}(x \given y)} \,\mathrm dy
                = \int \int p_{\theta}(x, y) q_{\phi}(x \given y) \,\mathrm dx \,\mathrm dy + \text{const.}
            \right\}
        \end{align}
        by performing \gls{SGD}: \\
        \For{some number of iterations}{
            Sample $(x^m, y^m) \sim p(x, y)$. \\
            Calculate the gradient $\nabla_{\phi} q_{\phi}(x^m \given y^m)$. \\
            Update $\phi$.
        }
    }
    \Return{$\theta, \phi$}
  }
  \caption{Coordinate Descent Auto-Encoder}
  \label{alg:algorithm/cdae}
\end{algorithm}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
